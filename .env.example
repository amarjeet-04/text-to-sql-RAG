# LLM Configuration
OPENAI_API_KEY=your-openai-api-key-here
LLM_MODEL=gpt-4

# PostgreSQL Database
DB_HOST=localhost
DB_PORT=5432
DB_USER=your-db-username
DB_PASSWORD=your-db-password
DB_NAME=your-db-name

# Vector DB (optional)
VECTOR_BACKEND=faiss
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX=text2sql-schema
PINECONE_NAMESPACE=default
PINECONE_METRIC=cosine
PINECONE_CLOUD=aws
PINECONE_REGION=us-east-1

# Performance / cache controls
APP_THREADPOOL_MAX_WORKERS=8
APP_FOREGROUND_MAX_WORKERS=4
SCHEMA_CACHE_TTL_SECONDS=21600
SCHEMA_CACHE_MAX_ENTRIES=32
RAG_CACHE_TTL_SECONDS=900
RAG_CACHE_MAX_ENTRIES=256
# Query-result cache (exact-match, thread-safe LRU+TTL)
QUERY_CACHE_TTL_SECONDS=300
QUERY_CACHE_MAX_SIZE=300
QUERY_CACHE_ENABLE_SEMANTIC=false
QUERY_CACHE_SEMANTIC_THRESHOLD=0.97
QUERY_RESULT_RELATIVE_TTL_SECONDS=60
QUERY_CACHE_ENABLE_FRESHNESS_MARKER=true
DB_TRANSIENT_RETRIES=1
DB_TRANSIENT_RETRY_BACKOFF_SECONDS=0.2

# Guarded pipeline LLM time budget
GUARDED_PIPELINE_BUDGET_S=12
GUARDED_INTENT_TIMEOUT_S=2.0
GUARDED_SQL_TIMEOUT_S=6.0
GUARDED_EVAL_TIMEOUT_S=1.8
GUARDED_FIX_TIMEOUT_S=2.5
GUARDED_SCHEMA_CHAR_BUDGET=16000
