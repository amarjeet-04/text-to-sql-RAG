"""
Evaluation Module - Measure SQL generation accuracy with ground truth data

Includes RAGAS-style metrics adapted for Text-to-SQL evaluation:
- Faithfulness: Does the SQL correctly represent the question intent?
- Answer Correctness: Do the results match ground truth?
- Context Relevance: Is the schema context used appropriately?
- SQL Quality: Syntax correctness, efficiency, and best practices
"""
import json
import re
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path

import pandas as pd


# ============================================================================
# RAGAS-Style Metrics for Text-to-SQL
# ============================================================================

@dataclass
class RAGASMetrics:
    """RAGAS-style metrics adapted for Text-to-SQL evaluation"""

    # Core RAGAS metrics (0-1 scale)
    faithfulness: float = 0.0  # Does SQL match question intent?
    answer_correctness: float = 0.0  # Do results match ground truth?
    context_relevance: float = 0.0  # Is schema used correctly?

    # SQL-specific metrics (0-1 scale)
    sql_validity: float = 0.0  # Syntactically correct SQL?
    execution_accuracy: float = 0.0  # Query runs without error?
    result_similarity: float = 0.0  # How similar are the results?

    # Composite score
    def overall_score(self) -> float:
        """Weighted average of all metrics"""
        weights = {
            'faithfulness': 0.20,
            'answer_correctness': 0.30,
            'context_relevance': 0.10,
            'sql_validity': 0.15,
            'execution_accuracy': 0.15,
            'result_similarity': 0.10,
        }
        return (
            self.faithfulness * weights['faithfulness'] +
            self.answer_correctness * weights['answer_correctness'] +
            self.context_relevance * weights['context_relevance'] +
            self.sql_validity * weights['sql_validity'] +
            self.execution_accuracy * weights['execution_accuracy'] +
            self.result_similarity * weights['result_similarity']
        )

    def to_dict(self) -> Dict[str, float]:
        return {
            'faithfulness': round(self.faithfulness, 3),
            'answer_correctness': round(self.answer_correctness, 3),
            'context_relevance': round(self.context_relevance, 3),
            'sql_validity': round(self.sql_validity, 3),
            'execution_accuracy': round(self.execution_accuracy, 3),
            'result_similarity': round(self.result_similarity, 3),
            'overall_score': round(self.overall_score(), 3),
        }


class RAGASEvaluator:
    """Evaluates Text-to-SQL using RAGAS-style metrics"""

    def __init__(self, llm=None):
        """
        Args:
            llm: Optional LLM for semantic evaluation (faithfulness, context)
        """
        self.llm = llm

    def evaluate(
        self,
        question: str,
        generated_sql: str,
        ground_truth_sql: str,
        generated_result: Optional[pd.DataFrame],
        ground_truth_result: Optional[pd.DataFrame],
        schema_context: str = "",
        execution_success: bool = False,
    ) -> RAGASMetrics:
        """
        Evaluate a single Text-to-SQL generation

        Args:
            question: Natural language question
            generated_sql: SQL generated by the model
            ground_truth_sql: Expected correct SQL
            generated_result: DataFrame from executing generated SQL
            ground_truth_result: DataFrame from executing ground truth SQL
            schema_context: Database schema used for generation
            execution_success: Whether the generated SQL executed without error

        Returns:
            RAGASMetrics with all evaluation scores
        """
        metrics = RAGASMetrics()

        # 1. SQL Validity (syntactic correctness)
        metrics.sql_validity = self._evaluate_sql_validity(generated_sql)

        # 2. Execution Accuracy
        metrics.execution_accuracy = 1.0 if execution_success else 0.0

        # 3. Answer Correctness (result comparison)
        metrics.answer_correctness = self._evaluate_answer_correctness(
            generated_result, ground_truth_result
        )

        # 4. Result Similarity (partial match score)
        metrics.result_similarity = self._evaluate_result_similarity(
            generated_result, ground_truth_result
        )

        # 5. Faithfulness (SQL matches question intent)
        metrics.faithfulness = self._evaluate_faithfulness(
            question, generated_sql, ground_truth_sql
        )

        # 6. Context Relevance (schema usage)
        metrics.context_relevance = self._evaluate_context_relevance(
            generated_sql, schema_context
        )

        return metrics

    def _evaluate_sql_validity(self, sql: str) -> float:
        """Check if SQL is syntactically valid"""
        if not sql or not sql.strip():
            return 0.0

        sql_upper = sql.upper().strip()

        # Check for basic SQL structure
        has_select = 'SELECT' in sql_upper
        has_from = 'FROM' in sql_upper

        # Check for common syntax errors
        balanced_parens = sql.count('(') == sql.count(')')
        balanced_quotes = sql.count("'") % 2 == 0

        # Check for markdown/explanation contamination
        has_markdown = '```' in sql or sql.strip().startswith('#')
        has_explanation = any(word in sql_upper for word in ['HERE IS', 'THE QUERY', 'THIS SQL'])

        if has_markdown or has_explanation:
            return 0.3

        if has_select and has_from and balanced_parens and balanced_quotes:
            return 1.0
        elif has_select and has_from:
            return 0.7
        elif has_select:
            return 0.3

        return 0.0

    def _evaluate_answer_correctness(
        self,
        generated: Optional[pd.DataFrame],
        ground_truth: Optional[pd.DataFrame]
    ) -> float:
        """Evaluate if results match ground truth exactly"""
        if generated is None or ground_truth is None:
            return 0.0

        if generated.empty and ground_truth.empty:
            return 1.0

        if generated.empty or ground_truth.empty:
            return 0.0

        # Check exact match (normalized)
        try:
            gen_sorted = self._normalize_df(generated)
            gt_sorted = self._normalize_df(ground_truth)

            if gen_sorted.shape != gt_sorted.shape:
                return 0.0

            # Compare values with tolerance
            match_count = 0
            total_cells = gen_sorted.size

            for col_idx in range(len(gen_sorted.columns)):
                col1 = gen_sorted.iloc[:, col_idx]
                col2 = gt_sorted.iloc[:, col_idx]

                for i in range(len(col1)):
                    if self._values_match(col1.iloc[i], col2.iloc[i]):
                        match_count += 1

            return match_count / total_cells if total_cells > 0 else 0.0

        except Exception:
            return 0.0

    def _evaluate_result_similarity(
        self,
        generated: Optional[pd.DataFrame],
        ground_truth: Optional[pd.DataFrame]
    ) -> float:
        """Evaluate partial similarity between results"""
        if generated is None or ground_truth is None:
            return 0.0

        if generated.empty and ground_truth.empty:
            return 1.0

        if generated.empty or ground_truth.empty:
            return 0.0

        try:
            scores = []

            # Column count similarity
            col_similarity = min(len(generated.columns), len(ground_truth.columns)) / \
                           max(len(generated.columns), len(ground_truth.columns))
            scores.append(col_similarity)

            # Row count similarity
            row_similarity = min(len(generated), len(ground_truth)) / \
                           max(len(generated), len(ground_truth))
            scores.append(row_similarity)

            # First column value overlap (key column)
            if len(generated.columns) > 0 and len(ground_truth.columns) > 0:
                gen_vals = set(str(v) for v in generated.iloc[:, 0].fillna(''))
                gt_vals = set(str(v) for v in ground_truth.iloc[:, 0].fillna(''))

                if gen_vals or gt_vals:
                    overlap = len(gen_vals & gt_vals)
                    union = len(gen_vals | gt_vals)
                    scores.append(overlap / union if union > 0 else 0)

            return sum(scores) / len(scores) if scores else 0.0

        except Exception:
            return 0.0

    def _evaluate_faithfulness(
        self,
        question: str,
        generated_sql: str,
        ground_truth_sql: str
    ) -> float:
        """Evaluate if SQL faithfully represents the question intent"""
        if not generated_sql:
            return 0.0

        # Extract key components from question
        question_lower = question.lower()
        sql_lower = generated_sql.lower()
        gt_lower = ground_truth_sql.lower()

        scores = []

        # Check aggregation alignment
        agg_keywords = {
            'total': ['sum(', 'count('],
            'average': ['avg('],
            'count': ['count('],
            'revenue': ['sum(', 'total_sales'],
            'profit': ['total_profit'],
            'bookings': ['total_booking'],
        }

        for keyword, sql_patterns in agg_keywords.items():
            if keyword in question_lower:
                has_pattern = any(p in sql_lower for p in sql_patterns)
                gt_has_pattern = any(p in gt_lower for p in sql_patterns)
                if gt_has_pattern:
                    scores.append(1.0 if has_pattern else 0.0)

        # Check filter alignment
        filter_keywords = ['confirmed', 'cancelled', 'pending']
        for keyword in filter_keywords:
            if keyword in question_lower:
                has_filter = keyword in sql_lower
                gt_has_filter = keyword in gt_lower
                if gt_has_filter:
                    scores.append(1.0 if has_filter else 0.0)

        # Check grouping alignment
        group_keywords = ['by agent', 'by country', 'by city', 'by supplier', 'by hotel', 'by chain', 'by nationality']
        for keyword in group_keywords:
            if keyword in question_lower:
                dimension = keyword.replace('by ', '')
                has_group = dimension in sql_lower and 'group by' in sql_lower
                scores.append(1.0 if has_group else 0.0)

        # Check date alignment
        date_keywords = ['this year', 'last year', 'this month', 'last 90 days', 'last 30 days']
        for keyword in date_keywords:
            if keyword in question_lower:
                has_date = 'current_date' in sql_lower or 'extract' in sql_lower or 'date_trunc' in sql_lower
                scores.append(1.0 if has_date else 0.0)

        return sum(scores) / len(scores) if scores else 0.5

    def _evaluate_context_relevance(self, sql: str, schema_context: str) -> float:
        """Evaluate if schema context is used appropriately"""
        if not sql:
            return 0.0

        sql_lower = sql.lower()

        # Check for common schema elements
        score = 0.5  # Base score

        # Positive: Uses correct table (any of the level views)
        view_names = ['agent_level_view', 'supplier_level_view', 'country_level_view',
                      'city_level_view', 'client_nationality_level_view', 'hotel_level_view',
                      'hotel_chain_level_view']
        if any(v in sql_lower for v in view_names):
            score += 0.2

        # Positive: Uses known columns
        known_columns = [
            'agentname', 'agentcountry', 'agentcity', 'total_sales', 'total_profit',
            'total_booking', 'country', 'city', 'booking_date', 'checkin_date',
            'checkout_date', 'suppliername', 'productname', 'hotelname', 'chain',
            'clientnationality'
        ]
        used_columns = sum(1 for col in known_columns if col in sql_lower)
        score += min(0.3, used_columns * 0.05)

        # Negative: Uses non-existent elements (hallucination)
        # This would need schema context to properly evaluate

        return min(1.0, score)

    def _normalize_df(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalize DataFrame for comparison"""
        result = df.copy()
        result.columns = [str(c).lower().strip() for c in result.columns]

        # Sort by first column
        try:
            result = result.sort_values(by=result.columns[0]).reset_index(drop=True)
        except Exception:
            pass

        return result

    def _values_match(self, v1, v2, tolerance: float = 0.01) -> bool:
        """Check if two values match with tolerance"""
        try:
            # Try numeric comparison
            f1, f2 = float(v1), float(v2)
            return abs(f1 - f2) < tolerance or abs(f1 - f2) / max(abs(f1), abs(f2), 1) < tolerance
        except (ValueError, TypeError):
            # String comparison
            return str(v1).strip().lower() == str(v2).strip().lower()


def calculate_ragas_summary(results: List['EvaluationResult']) -> Dict[str, Any]:
    """Calculate aggregate RAGAS metrics from evaluation results"""
    if not results:
        return {}

    total = len(results)

    # Aggregate RAGAS metrics
    ragas_metrics = {
        'avg_faithfulness': 0.0,
        'avg_answer_correctness': 0.0,
        'avg_context_relevance': 0.0,
        'avg_sql_validity': 0.0,
        'avg_execution_accuracy': 0.0,
        'avg_result_similarity': 0.0,
        'avg_ragas_score': 0.0,
    }

    for r in results:
        if hasattr(r, 'ragas_metrics') and r.ragas_metrics:
            metrics = r.ragas_metrics
            ragas_metrics['avg_faithfulness'] += metrics.faithfulness
            ragas_metrics['avg_answer_correctness'] += metrics.answer_correctness
            ragas_metrics['avg_context_relevance'] += metrics.context_relevance
            ragas_metrics['avg_sql_validity'] += metrics.sql_validity
            ragas_metrics['avg_execution_accuracy'] += metrics.execution_accuracy
            ragas_metrics['avg_result_similarity'] += metrics.result_similarity
            ragas_metrics['avg_ragas_score'] += metrics.overall_score()

    # Calculate averages
    for key in ragas_metrics:
        ragas_metrics[key] = round(ragas_metrics[key] / total, 3)

    return ragas_metrics


@dataclass
class EvaluationCase:
    """A single test case for evaluation"""
    question: str
    ground_truth_sql: str
    category: str = "general"  # e.g., aggregation, join, filter, date, text_search
    difficulty: str = "medium"  # easy, medium, hard
    description: str = ""
    expected_columns: List[str] = field(default_factory=list)
    expected_row_count: Optional[int] = None  # None means don't check


@dataclass
class EvaluationResult:
    """Result of evaluating a single case"""
    question: str
    ground_truth_sql: str
    generated_sql: str
    category: str
    difficulty: str

    # Metrics
    sql_exact_match: bool = False
    sql_semantic_match: bool = False  # Normalized SQL comparison
    execution_success: bool = False
    result_match: bool = False  # Results match ground truth
    column_match: bool = False
    row_count_match: bool = False

    # Details
    ground_truth_result: Optional[pd.DataFrame] = None
    generated_result: Optional[pd.DataFrame] = None
    error_message: str = ""
    execution_time_ms: float = 0.0

    # RAGAS metrics
    ragas_metrics: Optional[RAGASMetrics] = None

    def score(self) -> float:
        """Calculate overall score (0-1)

        Scoring priorities:
        1. Execution success - query runs without error (25%)
        2. Result match - data values are correct (40%)
        3. Row count match - same number of results (20%)
        4. Column match - same column names (15%)

        Note: Result match is most important as it means the data is correct
        even if column names differ slightly.
        """
        score = 0.0

        # Execution success (25%)
        if self.execution_success:
            score += 0.25

        # Result match is the most important (40%)
        if self.result_match:
            score += 0.40

        # Row count match (20%)
        if self.row_count_match:
            score += 0.20

        # Column name match (15%)
        if self.column_match:
            score += 0.15

        return score


class SQLEvaluator:
    """Evaluates generated SQL against ground truth"""

    def __init__(self, db, execute_fn=None):
        """
        Args:
            db: SQLDatabase instance
            execute_fn: Optional custom execution function (query) -> (df, error)
        """
        self.db = db
        self.execute_fn = execute_fn or self._default_execute
        self.results: List[EvaluationResult] = []

    def _default_execute(self, query: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
        """Default query execution"""
        try:
            import sqlalchemy
            engine = self.db._engine
            with engine.connect() as conn:
                result = conn.execute(sqlalchemy.text(query))
                columns = list(result.keys())
                rows = result.fetchall()
            if rows:
                return pd.DataFrame(rows, columns=columns), None
            return pd.DataFrame(), None
        except Exception as e:
            return None, str(e)

    def normalize_sql(self, sql: str) -> str:
        """Normalize SQL for comparison"""
        if not sql:
            return ""

        # Lowercase
        sql = sql.lower()

        # Remove extra whitespace
        sql = re.sub(r'\s+', ' ', sql)

        # Remove trailing semicolon
        sql = sql.rstrip(';').strip()

        # Normalize quotes
        sql = sql.replace('"', "'")

        # Remove comments
        sql = re.sub(r'--.*$', '', sql, flags=re.MULTILINE)
        sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)

        # Normalize common variations
        sql = re.sub(r'\s*,\s*', ', ', sql)
        sql = re.sub(r'\s*=\s*', ' = ', sql)
        sql = re.sub(r'\(\s+', '(', sql)
        sql = re.sub(r'\s+\)', ')', sql)

        return sql.strip()

    def compare_results(
        self,
        df1: Optional[pd.DataFrame],
        df2: Optional[pd.DataFrame],
        check_order: bool = False
    ) -> Tuple[bool, bool, bool]:
        """
        Compare two DataFrames with flexible matching.

        Returns:
            (columns_match, values_match, row_count_match)

        Matching rules:
        - Column names can differ (aliases) as long as data types match
        - Values are compared with tolerance for floats
        - Row order doesn't matter unless check_order=True
        """
        if df1 is None or df2 is None:
            return False, False, False

        if df1.empty and df2.empty:
            return True, True, True

        if df1.empty or df2.empty:
            return False, False, False

        # Check column count match (more flexible than exact name match)
        cols1 = list(df1.columns)
        cols2 = list(df2.columns)

        # Exact column name match
        cols1_normalized = set(c.lower().strip() for c in cols1)
        cols2_normalized = set(c.lower().strip() for c in cols2)
        columns_match = cols1_normalized == cols2_normalized

        # Check row count (allow small differences due to LIMIT variations)
        row_count_match = len(df1) == len(df2)
        row_count_close = abs(len(df1) - len(df2)) <= max(5, int(len(df1) * 0.1))

        # Check values with flexible matching
        values_match = False

        # Strategy 1: Exact column match comparison
        if columns_match and row_count_match:
            values_match = self._compare_values_exact(df1, df2, check_order)

        # Strategy 2: If columns differ but same count, try positional matching
        if not values_match and len(cols1) == len(cols2) and row_count_close:
            values_match = self._compare_values_positional(df1, df2, check_order)

        # Strategy 3: Check if key columns (first column) have same values
        if not values_match and row_count_close:
            values_match = self._compare_key_column(df1, df2)

        return columns_match, values_match, row_count_match

    def _compare_values_exact(self, df1: pd.DataFrame, df2: pd.DataFrame, check_order: bool) -> bool:
        """Compare DataFrames with exact column name matching"""
        try:
            df1_sorted = df1.copy()
            df2_sorted = df2.copy()

            # Normalize column names
            df1_sorted.columns = [c.lower().strip() for c in df1_sorted.columns]
            df2_sorted.columns = [c.lower().strip() for c in df2_sorted.columns]

            # Sort by common columns
            sort_cols = list(df1_sorted.columns)
            if not check_order and sort_cols:
                try:
                    df1_sorted = df1_sorted.sort_values(by=sort_cols).reset_index(drop=True)
                    df2_sorted = df2_sorted.sort_values(by=sort_cols).reset_index(drop=True)
                except Exception:
                    pass  # Skip sort if fails

            # Compare with tolerance
            for col in df1_sorted.columns:
                if col not in df2_sorted.columns:
                    return False
                if df1_sorted[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                    try:
                        diff = abs(df1_sorted[col].fillna(0).astype(float) - df2_sorted[col].fillna(0).astype(float))
                        if not (diff < 0.01).all():
                            return False
                    except Exception:
                        return False
                else:
                    if not (df1_sorted[col].fillna('').astype(str) ==
                            df2_sorted[col].fillna('').astype(str)).all():
                        return False
            return True
        except Exception:
            return False

    def _compare_values_positional(self, df1: pd.DataFrame, df2: pd.DataFrame, check_order: bool) -> bool:
        """Compare DataFrames by column position (ignore column names)"""
        try:
            # Use min rows if counts differ slightly
            min_rows = min(len(df1), len(df2))
            df1_trimmed = df1.head(min_rows).reset_index(drop=True)
            df2_trimmed = df2.head(min_rows).reset_index(drop=True)

            # Sort both by first column if not checking order
            if not check_order:
                try:
                    df1_trimmed = df1_trimmed.sort_values(by=df1_trimmed.columns[0]).reset_index(drop=True)
                    df2_trimmed = df2_trimmed.sort_values(by=df2_trimmed.columns[0]).reset_index(drop=True)
                except Exception:
                    pass

            # Compare column by column positionally
            for i in range(len(df1_trimmed.columns)):
                col1 = df1_trimmed.iloc[:, i]
                col2 = df2_trimmed.iloc[:, i]

                # Try numeric comparison
                try:
                    diff = abs(col1.fillna(0).astype(float) - col2.fillna(0).astype(float))
                    if not (diff < 0.01).all():
                        return False
                except (ValueError, TypeError):
                    # String comparison
                    if not (col1.fillna('').astype(str) == col2.fillna('').astype(str)).all():
                        return False
            return True
        except Exception:
            return False

    def _compare_key_column(self, df1: pd.DataFrame, df2: pd.DataFrame) -> bool:
        """Check if the first (key) column has the same unique values"""
        try:
            min_rows = min(len(df1), len(df2))
            vals1 = set(df1.iloc[:min_rows, 0].fillna('').astype(str))
            vals2 = set(df2.iloc[:min_rows, 0].fillna('').astype(str))

            # Check overlap percentage
            overlap = len(vals1 & vals2)
            total = len(vals1 | vals2)

            if total == 0:
                return True

            # Consider match if >80% overlap
            return (overlap / total) >= 0.8
        except Exception:
            return False

    def evaluate_case(self, case: EvaluationCase, generated_sql: str) -> EvaluationResult:
        """Evaluate a single test case"""
        import time

        result = EvaluationResult(
            question=case.question,
            ground_truth_sql=case.ground_truth_sql,
            generated_sql=generated_sql,
            category=case.category,
            difficulty=case.difficulty,
        )

        # Check SQL match
        norm_gt = self.normalize_sql(case.ground_truth_sql)
        norm_gen = self.normalize_sql(generated_sql)
        result.sql_exact_match = norm_gt == norm_gen
        result.sql_semantic_match = self._semantic_sql_match(norm_gt, norm_gen)

        # Execute ground truth
        gt_df, gt_error = self.execute_fn(case.ground_truth_sql)
        if gt_error:
            result.error_message = f"Ground truth error: {gt_error}"
            return result
        result.ground_truth_result = gt_df

        # Execute generated SQL
        start_time = time.time()
        gen_df, gen_error = self.execute_fn(generated_sql)
        result.execution_time_ms = (time.time() - start_time) * 1000

        if gen_error:
            result.error_message = f"Generated SQL error: {gen_error}"
            result.execution_success = False
            return result

        result.execution_success = True
        result.generated_result = gen_df

        # Compare results
        col_match, val_match, row_match = self.compare_results(gt_df, gen_df)
        result.column_match = col_match
        result.result_match = val_match
        result.row_count_match = row_match

        # Calculate RAGAS metrics
        ragas_evaluator = RAGASEvaluator()
        result.ragas_metrics = ragas_evaluator.evaluate(
            question=case.question,
            generated_sql=generated_sql,
            ground_truth_sql=case.ground_truth_sql,
            generated_result=gen_df,
            ground_truth_result=gt_df,
            execution_success=result.execution_success,
        )

        return result

    def _semantic_sql_match(self, sql1: str, sql2: str) -> bool:
        """Check if two SQL statements are semantically similar"""
        if sql1 == sql2:
            return True

        # Extract key components
        def extract_components(sql):
            components = {
                "select": set(),
                "from": set(),
                "where": set(),
                "group_by": set(),
                "order_by": set(),
            }

            # Simple extraction (could be enhanced with SQL parser)
            select_match = re.search(r'select\s+(.*?)\s+from', sql, re.IGNORECASE)
            if select_match:
                cols = select_match.group(1).split(',')
                components["select"] = set(c.strip() for c in cols)

            from_match = re.search(r'from\s+(\w+)', sql, re.IGNORECASE)
            if from_match:
                components["from"].add(from_match.group(1))

            where_match = re.search(r'where\s+(.*?)(?:group|order|limit|$)', sql, re.IGNORECASE)
            if where_match:
                components["where"].add(where_match.group(1).strip())

            group_match = re.search(r'group\s+by\s+(.*?)(?:having|order|limit|$)', sql, re.IGNORECASE)
            if group_match:
                cols = group_match.group(1).split(',')
                components["group_by"] = set(c.strip() for c in cols)

            return components

        comp1 = extract_components(sql1)
        comp2 = extract_components(sql2)

        # Check if key components match
        return (comp1["from"] == comp2["from"] and
                comp1["group_by"] == comp2["group_by"])

    def run_evaluation(
        self,
        cases: List[EvaluationCase],
        sql_generator_fn
    ) -> Dict[str, Any]:
        """
        Run evaluation on multiple cases

        Args:
            cases: List of evaluation cases
            sql_generator_fn: Function that takes question and returns SQL

        Returns:
            Evaluation summary with metrics
        """
        self.results = []

        for case in cases:
            try:
                generated_sql = sql_generator_fn(case.question)
                result = self.evaluate_case(case, generated_sql)
                self.results.append(result)
            except Exception as e:
                result = EvaluationResult(
                    question=case.question,
                    ground_truth_sql=case.ground_truth_sql,
                    generated_sql="",
                    category=case.category,
                    difficulty=case.difficulty,
                    error_message=f"Generation error: {str(e)}"
                )
                self.results.append(result)

        return self.get_summary()

    def get_summary(self) -> Dict[str, Any]:
        """Get evaluation summary"""
        if not self.results:
            return {"error": "No results to summarize"}

        total = len(self.results)

        # Overall metrics
        summary = {
            "total_cases": total,
            "execution_success_rate": sum(r.execution_success for r in self.results) / total,
            "result_match_rate": sum(r.result_match for r in self.results) / total,
            "column_match_rate": sum(r.column_match for r in self.results) / total,
            "sql_exact_match_rate": sum(r.sql_exact_match for r in self.results) / total,
            "sql_semantic_match_rate": sum(r.sql_semantic_match for r in self.results) / total,
            "average_score": sum(r.score() for r in self.results) / total,
            "avg_execution_time_ms": sum(r.execution_time_ms for r in self.results) / total,
        }

        # Add RAGAS metrics
        summary["ragas_metrics"] = calculate_ragas_summary(self.results)

        # By category
        categories = set(r.category for r in self.results)
        summary["by_category"] = {}
        for cat in categories:
            cat_results = [r for r in self.results if r.category == cat]
            cat_total = len(cat_results)
            summary["by_category"][cat] = {
                "count": cat_total,
                "execution_success_rate": sum(r.execution_success for r in cat_results) / cat_total,
                "result_match_rate": sum(r.result_match for r in cat_results) / cat_total,
                "average_score": sum(r.score() for r in cat_results) / cat_total,
            }

        # By difficulty
        difficulties = set(r.difficulty for r in self.results)
        summary["by_difficulty"] = {}
        for diff in difficulties:
            diff_results = [r for r in self.results if r.difficulty == diff]
            diff_total = len(diff_results)
            summary["by_difficulty"][diff] = {
                "count": diff_total,
                "execution_success_rate": sum(r.execution_success for r in diff_results) / diff_total,
                "result_match_rate": sum(r.result_match for r in diff_results) / diff_total,
                "average_score": sum(r.score() for r in diff_results) / diff_total,
            }

        # Failed cases
        summary["failed_cases"] = [
            {
                "question": r.question,
                "error": r.error_message,
                "generated_sql": r.generated_sql[:200] if r.generated_sql else "",
            }
            for r in self.results
            if not r.execution_success or not r.result_match
        ]

        return summary

    def export_results(self, filepath: str):
        """Export results to JSON"""
        export_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": self.get_summary(),
            "detailed_results": [
                {
                    "question": r.question,
                    "category": r.category,
                    "difficulty": r.difficulty,
                    "ground_truth_sql": r.ground_truth_sql,
                    "generated_sql": r.generated_sql,
                    "sql_exact_match": r.sql_exact_match,
                    "sql_semantic_match": r.sql_semantic_match,
                    "execution_success": r.execution_success,
                    "result_match": r.result_match,
                    "column_match": r.column_match,
                    "row_count_match": r.row_count_match,
                    "score": r.score(),
                    "execution_time_ms": r.execution_time_ms,
                    "error_message": r.error_message,
                }
                for r in self.results
            ]
        }

        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)


def load_evaluation_cases(filepath: str) -> List[EvaluationCase]:
    """Load evaluation cases from JSON file"""
    with open(filepath, 'r') as f:
        data = json.load(f)

    return [
        EvaluationCase(
            question=case["question"],
            ground_truth_sql=case["ground_truth_sql"],
            category=case.get("category", "general"),
            difficulty=case.get("difficulty", "medium"),
            description=case.get("description", ""),
            expected_columns=case.get("expected_columns", []),
            expected_row_count=case.get("expected_row_count"),
        )
        for case in data.get("cases", data)  # Support both {cases: [...]} and [...]
    ]


def create_sample_evaluation_cases() -> List[EvaluationCase]:
    """Create sample evaluation cases for testing.

    Uses actual database tables (BookingData, AgentMaster, HotelData, etc.)
    with MSSQL syntax (TOP N, GETDATE, DATEDIFF, LIKE).
    """
    return [
        # --- Aggregation queries ---
        EvaluationCase(
            question="What is the total revenue?",
            ground_truth_sql="SELECT SUM(AgentSellingPrice) AS total_revenue FROM BookingData",
            category="aggregation",
            difficulty="easy",
            description="Simple sum of selling price across all bookings"
        ),
        EvaluationCase(
            question="Show total revenue by agent",
            ground_truth_sql=(
                "SELECT a.AgentName, SUM(b.AgentSellingPrice) AS revenue "
                "FROM BookingData b JOIN AgentMaster a ON b.AgentId = a.AgentId "
                "GROUP BY a.AgentName ORDER BY revenue DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="Revenue aggregation by agent with JOIN"
        ),
        EvaluationCase(
            question="How many bookings are there in total?",
            ground_truth_sql="SELECT COUNT(*) AS total_bookings FROM BookingData",
            category="aggregation",
            difficulty="easy",
            description="Simple count of all bookings"
        ),
        EvaluationCase(
            question="What is the average selling price per booking?",
            ground_truth_sql="SELECT AVG(AgentSellingPrice) AS avg_selling_price FROM BookingData",
            category="aggregation",
            difficulty="easy",
            description="Average selling price calculation"
        ),

        # --- Filter queries ---
        EvaluationCase(
            question="Find all bookings for India as destination",
            ground_truth_sql=(
                "SELECT COUNT(*) AS bookings, SUM(AgentSellingPrice) AS revenue "
                "FROM BookingData b JOIN Master_Country mc ON b.DestinationCountryId = mc.CountryId "
                "WHERE mc.CountryName LIKE '%India%'"
            ),
            category="filter",
            difficulty="easy",
            description="Text search on destination country"
        ),
        EvaluationCase(
            question="Show all confirmed bookings",
            ground_truth_sql=(
                "SELECT COUNT(*) AS confirmed_bookings, SUM(AgentSellingPrice) AS revenue "
                "FROM BookingData WHERE BookingStatus = 'Confirmed'"
            ),
            category="filter",
            difficulty="easy",
            description="Filter by booking status"
        ),
        EvaluationCase(
            question="Find bookings where selling price is greater than 10000",
            ground_truth_sql=(
                "SELECT COUNT(*) AS booking_count, SUM(AgentSellingPrice) AS total_revenue "
                "FROM BookingData WHERE AgentSellingPrice > 10000"
            ),
            category="filter",
            difficulty="easy",
            description="Numeric filter on selling price"
        ),

        # --- Date queries ---
        EvaluationCase(
            question="Show bookings from the last 30 days",
            ground_truth_sql=(
                "SELECT COUNT(*) AS bookings, SUM(AgentSellingPrice) AS revenue "
                "FROM BookingData WHERE CreatedDate >= DATEADD(DAY, -30, GETDATE())"
            ),
            category="date",
            difficulty="medium",
            description="Date range filter using DATEADD"
        ),
        EvaluationCase(
            question="How many bookings were created this year?",
            ground_truth_sql=(
                "SELECT COUNT(*) AS bookings_this_year "
                "FROM BookingData WHERE YEAR(CreatedDate) = YEAR(GETDATE())"
            ),
            category="date",
            difficulty="medium",
            description="Year-based date filter"
        ),
        EvaluationCase(
            question="Show monthly booking count for the current year",
            ground_truth_sql=(
                "SELECT MONTH(CreatedDate) AS month, COUNT(*) AS booking_count "
                "FROM BookingData WHERE YEAR(CreatedDate) = YEAR(GETDATE()) "
                "GROUP BY MONTH(CreatedDate) ORDER BY month"
            ),
            category="date",
            difficulty="medium",
            description="Monthly aggregation with date functions"
        ),

        # --- JOIN queries ---
        EvaluationCase(
            question="What is the average stay duration per agent?",
            ground_truth_sql=(
                "SELECT a.AgentName, "
                "AVG(DATEDIFF(DAY, b.CheckInDate, b.CheckOutDate)) AS avg_stay_days, "
                "COUNT(*) AS booking_count "
                "FROM BookingData b JOIN AgentMaster a ON b.AgentId = a.AgentId "
                "GROUP BY a.AgentName ORDER BY avg_stay_days DESC"
            ),
            category="aggregation",
            difficulty="hard",
            description="Average stay duration with DATEDIFF and JOIN"
        ),
        EvaluationCase(
            question="Show bookings by hotel chain",
            ground_truth_sql=(
                "SELECT hc.HotelChainName, COUNT(*) AS bookings, "
                "SUM(b.AgentSellingPrice) AS revenue "
                "FROM BookingData b JOIN HotelChain hc ON b.HotelChainId = hc.HotelChainId "
                "GROUP BY hc.HotelChainName ORDER BY revenue DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="Aggregation with hotel chain JOIN"
        ),

        # --- Top N queries ---
        EvaluationCase(
            question="Top 5 agents by revenue",
            ground_truth_sql=(
                "SELECT TOP 5 a.AgentName, a.AgentCountry, "
                "SUM(b.AgentSellingPrice) AS revenue, COUNT(*) AS bookings "
                "FROM BookingData b JOIN AgentMaster a ON b.AgentId = a.AgentId "
                "GROUP BY a.AgentName, a.AgentCountry ORDER BY revenue DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="Top N agents by revenue"
        ),
        EvaluationCase(
            question="Top 10 hotels by number of bookings",
            ground_truth_sql=(
                "SELECT TOP 10 h.HotelName, COUNT(*) AS booking_count "
                "FROM BookingData b JOIN HotelData h ON b.HotelId = h.HotelId "
                "GROUP BY h.HotelName ORDER BY booking_count DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="Top N hotels by booking count"
        ),

        # --- Complex / multi-table queries ---
        EvaluationCase(
            question="Show revenue by destination country",
            ground_truth_sql=(
                "SELECT mc.CountryName, SUM(b.AgentSellingPrice) AS revenue, "
                "COUNT(*) AS bookings "
                "FROM BookingData b JOIN Master_Country mc ON b.DestinationCountryId = mc.CountryId "
                "GROUP BY mc.CountryName ORDER BY revenue DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="Revenue aggregation by destination country"
        ),
        EvaluationCase(
            question="Show profit margin by product type",
            ground_truth_sql=(
                "SELECT ProductName, "
                "SUM(AgentSellingPrice) AS revenue, "
                "SUM(CompanyBuyingPrice) AS cost, "
                "SUM(AgentSellingPrice) - SUM(CompanyBuyingPrice) AS profit "
                "FROM BookingData GROUP BY ProductName ORDER BY profit DESC"
            ),
            category="aggregation",
            difficulty="hard",
            description="Profit margin calculation by product"
        ),
        EvaluationCase(
            question="Which city has the most bookings?",
            ground_truth_sql=(
                "SELECT TOP 1 mc.CityName, COUNT(*) AS booking_count "
                "FROM BookingData b JOIN Master_City mc ON b.CityId = mc.CityId "
                "GROUP BY mc.CityName ORDER BY booking_count DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="City with highest booking count"
        ),
        EvaluationCase(
            question="Show cancelled bookings with agent details",
            ground_truth_sql=(
                "SELECT a.AgentName, a.AgentCountry, COUNT(*) AS cancelled_count, "
                "SUM(b.AgentSellingPrice) AS lost_revenue "
                "FROM BookingData b JOIN AgentMaster a ON b.AgentId = a.AgentId "
                "WHERE b.BookingStatus = 'Cancelled' "
                "GROUP BY a.AgentName, a.AgentCountry ORDER BY cancelled_count DESC"
            ),
            category="filter",
            difficulty="hard",
            description="Cancelled bookings with agent JOIN and aggregation"
        ),
        EvaluationCase(
            question="Show booking count by service type",
            ground_truth_sql=(
                "SELECT st.ServiceTypeName, COUNT(*) AS bookings "
                "FROM BookingData b JOIN Master_ServiceType st ON b.ServiceTypeId = st.ServiceTypeId "
                "GROUP BY st.ServiceTypeName ORDER BY bookings DESC"
            ),
            category="aggregation",
            difficulty="medium",
            description="Booking count by service type"
        ),
    ]
